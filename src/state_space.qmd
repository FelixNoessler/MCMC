---
title: "Simple State Space Model - MCMC and Filtering"
engine: julia
bibliography: lit.bib
format:
    html:
        toc: true
        number-sections: true
---


```{dot}
//| fig-width: 7
//| label: fig-hmm
//| fig-cap: "Forward algorithm for a hidden markov model."
digraph HMM {
    node [shape=circle, style=filled, width=0.8];
    
    S1 [label="s(t-1)", fillcolor=white];
    S2 [label="s(t)", fillcolor=white];
    S3 [label="s(t+1)", fillcolor=white];
    Send [label="s(T)", fillcolor=white];

    Xstart [style=invis];
    X0[label="x(0)", fillcolor=lightblue]
    X1 [label="x(t-1)", fillcolor=lightblue];
    X2 [label="x(t)", fillcolor=lightblue];
    X3 [label="x(t+1)", fillcolor=lightblue];
    Xend [label="x(T)", fillcolor=lightblue];

    Y1 [label="y(t-1)", fillcolor=salmon];
    Y2 [label="y(t)", fillcolor=salmon];
    Y3 [label="y(t+1)", fillcolor=salmon];
    Yend [label="y(T)", fillcolor=salmon];

    S1 -> X1 [label = "εₜ₋₁"];
    S2 -> X2;
    S3 -> X3;
    Send -> Xend;

    Xstart -> X0 [label="p(x₀)"];
    X0 -> S1 [label="                                       ", style=dashed];  
    X1 -> S2 [label = "p(xₜ | xₜ₋₁)"];
    X2 -> S3;
    X3 -> Send [label="                                       ", style=dashed];

    X1 -> Y1  [label = "ηₜ₋₁"];
    X2 -> Y2 [label="p(yₜ | xₜ)" ];
    X3 -> Y3; 
    Xend -> Yend;

    { rank=same; S1; S2; S3; Send; }
    { rank=same; Xstart; X0; X1; X2; X3; Xend; }
    { rank=same; Y1; Y2; Y3; Yend; }
}
```


- $s(t)$: state that is modelled with a process-based model at time $t$
- $x(t)$: true hidden state at time $t$
- $y(t)$: observation at time $t$

- $p(x(t) | x(t-1))$: transition probability between the true states
- $p(y(t) | x(t))$: observation probability
- $p(x₀)$: initial state of the true states
- $εₜ₋₁$: process error
- $ηₜ₋₁$: observation error


## Load packages and Makie theme
```{julia}
import Random

using CairoMakie
using Distributions
using LinearAlgebra
using LogDensityProblems
using ProtoStructs
using StatsBase
using Statistics
using TransformVariables
using TransformedLogDensities
using UnPack

set_theme!(
    fontsize = 18,
    Axis = (; xgridvisible = false, ygridvisible = false,
            topspinevisible = false, rightspinevisible = false),
    Legend = (; framevisible = false))
```

## Generate some data

```{julia}
Random.seed!(123)

σ_p = 2.0
σ_o = 3.0
β = 0.9
x₀ = 5.0
ε_t_dist = Normal(0, σ_p)
η_t_dist = Normal(0, σ_o)

ts = 1:200

s = Array{Float64}(undef, length(ts))
s_onestep = Array{Float64}(undef, length(ts))
x = Array{Float64}(undef, length(ts))
y = Array{Float64}(undef, length(ts))

for t in ts
    x_lastt = t == 1 ? x₀ : x[t-1]
    s_lastt = t == 1 ? x₀ : s[t-1]

    ε_t = rand(ε_t_dist)
    s[t] = β * s_lastt 
    s_onestep[t] = β * x_lastt
    x[t] = s_onestep[t] + ε_t
    
    η_t = rand(η_t_dist)    
    y[t] = x[t] + η_t
end

let
    fig = Figure(size = (900, 400))
    ax = Axis(fig[1, 1]; xlabel = "time", ylabel = "value")
    lines!(ts, s, color = :grey, label = "process-model state: s")
    lines!(ts, s_onestep, color = :grey, linestyle = :dash, 
        label = "one step ahead prediction: s",)
    scatterlines!(ts, x, color = :blue, label = "true hidden state: x")
    scatter!(ts, y, color = :red, label = "observations: y")
    Legend(fig[1, 2], ax)
    fig
end
```

## Particle filter - forward algorithm

```{julia}
@proto struct Particle
    val::Vector{Float64}
    log_weights::Vector{Float64}
    weights::Weights{Float64, Float64, Vector{Float64}}
end
    
function particle_filter(θ, problem; num_particles = 130)
    @unpack β, σ_p, σ_o, x₀ = θ
    @unpack y, ts, samples_hidden_state = problem


    particles = Particle(fill(x₀, num_particles), 
                         zeros(num_particles), 
                         Weights(ones(num_particles) / num_particles))

    ll = zeros(length(ts))
    
    for t in ts
        samples_hidden_state[t] = mean(particles.val)
         
        indices = sample(1:num_particles, particles.weights, num_particles; replace = true, ordered = true)
        
        
        
        particles.val .= particles.val[indices]
  
        particles.val .= β .* particles.val .+ rand(Normal(0.0, σ_p), num_particles)

        particles.log_weights .= logpdf.(Normal.(particles.val, σ_o), y[t])
        
        l = exp.(particles.log_weights .- maximum(particles.log_weights))
        if any(isnan.(l) .|| isinf.(sum(l)))   
            particles.weights .= Weights(ones(num_particles) / num_particles)
        else
            particles.weights .= Weights(l)
        end
        
        
        ll[t] = mean(particles.weights)
    end

    return sum(ll)
end
```

## Define problem

```{julia}	
my_priors = (;
    β = Normal(1, 0.2),
    x₀ = truncated(Normal(0, 10); lower = 0),
    σ_p = truncated(Normal(0, 1); lower = 0),
    σ_o = truncated(Normal(0, 1); lower = 0)
)

@proto struct StateSpaceModel
    ts::UnitRange{Int64} 
    y::Vector{Float64}
    prior_dists::NamedTuple
    nparameter::Int64
    transformation
    samples_hidden_state::Vector{Float64}
end

function Base.show(io::IO, problem::StateSpaceModel)
    println(io, "StateSpaceModel with $(problem.nparameter) parameters")
end

function (problem::StateSpaceModel)(θ)
    @unpack prior_dists = problem

    logprior = 0.0
    for keys in keys(prior_dists)
        logprior += logpdf(prior_dists[keys], θ[keys])
    end

    loglikelihood = particle_filter(θ, problem)
    
    return loglikelihood + logprior
end

function sample_prior(problem; inverse = true)
    @unpack prior_dists = problem
    θ = []
    for key in keys(prior_dists)
        push!(θ, rand(prior_dists[key]))
    end
    
    p = (; zip(keys(prior_dists), θ)...)

    if inverse
        p = TransformVariables.inverse(problem.transformation, p)
    end

    return p
end

my_transform = as((β = asℝ, x₀ = asℝ₊, σ_p = asℝ₊, σ_o = asℝ₊))
problem = StateSpaceModel(ts, y, my_priors, length(my_priors), my_transform, zeros(length(ts)))
ℓ = TransformedLogDensity(problem.transformation, problem)
posterior(x) = LogDensityProblems.logdensity(ℓ, x)
@show posterior(rand(problem.nparameter))
problem
```

## MCMC Sampler

```{julia}
function metroplis_sampler(problem; nsamples = 50_000, nchains = 4, 
                           propσ = [0.01, 0.01, 0.1, 0.01])
    @unpack nparameter, ts, transformation = problem

    
    accepted_θ = zeros(nchains, nparameter, nsamples)
    samples_hidden_state = zeros(nchains, nsamples, length(ts))

    accepted = zeros(nchains)

    Threads.@threads for n in 1:nchains
        θ = sample_prior(problem)
        post = posterior(θ)

        for k in 1:nsamples
            ## new proposal
            proposal_dist = MvNormal(θ, Diagonal(propσ))
            θstar = rand(proposal_dist) 
            
            ## evaluate prior + likelihood
            poststar = posterior(θstar)
            
            ## M-H ratio
            ratio = poststar - post

            if log(rand()) < min(ratio, 1)
                accepted[n] += 1
                θ = θstar
                post = poststar
            end
            
            accepted_θ[n, :, k] = θ
            samples_hidden_state[n, k, :] = problem.samples_hidden_state
        end
    end

    posterior_mat = zeros(nchains, nparameter, nsamples)
    for c in 1:nchains
        for i in 1:nsamples
            posterior_mat[c, :, i] = collect(transform(transformation, accepted_θ[c, :, i])) 
        end
    end

    return accepted_θ, posterior_mat, samples_hidden_state
end

```

## Run MCMC

```{julia}
raw_post, post, hidden_state = metroplis_sampler(problem; nsamples = 50000, propσ = [0.1, 0.1, 0.1, 0.1]);
```

```{julia}
let
    nchains, nparameter, nsamples = size(post)
    draws = 1:nsamples
    draw_start = max(Int(round(nsamples * 0.5)), 1)
    chains = 1:nchains

    fig = Figure(; size = (500, 800))

    for i in 1:nparameter
        parameter_name = string(keys(problem.prior_dists)[i])
        ax = Axis(fig[i, 1], ylabel = parameter_name, xticklabelsvisible = false)

        for c in chains
            lines!(ax, draws[draw_start:end], post[c, i, draw_start:end])
        end

        if i == nparameter
            ax.xlabel = "draw"
            ax.xticklabelsvisible = true
        end

        ax = Axis(fig[i, 2], yticklabelsvisible = false)

        for c in chains
            density!(ax, post[c, i, draw_start:end])
        end

        pdist = problem.prior_dists[i]
        plot!(pdist, color = :red, linewidth = 2)
    end

    fig
end
```

## Predictions

```{julia}
function sample_posterior(data)
    nchains, nparameter, nsamples = size(data)
    samples_start = Int(round(nsamples * 0.8))
    
    return data[sample(1:nchains), :, sample(samples_start:nsamples)]
end

function predict(data, problem; noise = false)
    @unpack ts = problem
    β, x₀, σ_p, σ_o = sample_posterior(data)

    x = Array{Float64}(undef, length(ts))

    for t in ts
        x_lastt = t == 1 ? x₀ : x[t-1]
        ε_t = rand(Normal(0, σ_p))

        if noise
            x[t] = β * x_lastt + ε_t
        else
            x[t] = β * x_lastt
        end
    end

    return x
end


function sample_hidden_state(data)
    nchains, nsamples, nts = size(data)
    samples_start = Int(round(nsamples * 0.5))
    
    return data[sample(1:nchains), sample(samples_start:nsamples), :]
end


let
    fig = Figure(size = (900, 800))
  
    Axis(fig[1, 1]; xlabel = "time", ylabel = "value")
    for i in 1:2
        x = sample_hidden_state(hidden_state)
        scatter!(ts .- 4, x; color = (:black, 0.1))
    end
    scatterlines!(ts, x, color = :blue, label = "true hidden state: x")

    Axis(fig[2, 1]; xlabel = "time", ylabel = "value")
    for i in 1:20
        x = predict(post, problem)
        lines!(ts, x; color = (:black, 0.1))
    end
    lines!(ts, s, color = :blue, label = "process-model state: s")


    fig
end
```